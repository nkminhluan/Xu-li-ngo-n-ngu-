{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Here are two reasons companies fail: they only do more of the same, or they only do what's new\", 'To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation', 'Both are necessary, but it can be too much of a good thing', 'Consider Facit', \"I'm actually old enough to remember them\", 'Facit was a fantastic company', 'Everybody used them', 'And what did Facit do when the electronic calculator came along? They continued doing exactly the same', 'In six months, they went from maximum revenue', 'and they were gone']\n"
     ]
    }
   ],
   "source": [
    "# đọc file\n",
    "filename='train.txt'\n",
    "lines=[]\n",
    "count=0\n",
    "#Max=-1\n",
    "Max=10000\n",
    "with open(filename,'r') as f:\n",
    "    for s in f:\n",
    "        count+=1\n",
    "        if count>Max and Max!=-1:\n",
    "            break\n",
    "        lines.append(s.strip())\n",
    "#print(len(lines))\n",
    "print(lines[:10])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_tokens_count= 185925\n",
      "10000\n",
      "[['<s>', 'here', 'are', 'two', 'reasons', 'companies', 'fail', ':', 'they', 'only', 'do', 'more', 'of', 'the', 'same', ',', 'or', 'they', 'only', 'do', 'what', \"'s\", 'new', '</s>'], ['<s>', 'to', 'me', 'the', 'real', ',', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', ':', 'exploration', 'and', 'exploitation', '</s>'], ['<s>', 'both', 'are', 'necessary', ',', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing', '</s>'], ['<s>', 'consider', 'facit', '</s>'], ['<s>', 'i', \"'m\", 'actually', 'old', 'enough', 'to', 'remember', 'them', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences \n",
    "import nltk\n",
    "sentences=[]\n",
    "all_tokens_count=0\n",
    "for line in lines:\n",
    "    tokens = nltk.word_tokenize(line.lower())\n",
    "    all_tokens_count+=len(tokens)\n",
    "    #sentences.append(tokens)\n",
    "    sentences.append(['<s>']+tokens+['</s>'])\n",
    "print('all_tokens_count=',all_tokens_count)\n",
    "print(len(sentences))\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V= 12838\n",
      "n= 185925\n",
      "8142\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "# counting 1-gram \n",
    "from collections import Counter\n",
    "counter_unigram=Counter()\n",
    "for sent in sentences:\n",
    "    counter_unigram.update(sent)\n",
    "V=len(counter_unigram)\n",
    "print('V=',V)\n",
    "n=0\n",
    "for gram in counter_unigram:\n",
    "    n+=counter_unigram[gram]\n",
    "n=n-counter_unigram['<s>']-counter_unigram['</s>']\n",
    "print('n=',n)\n",
    "print(counter_unigram['the'])\n",
    "print(counter_unigram['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195925\n",
      "('<s>', 'here')\n",
      "('here', 'are')\n",
      "('are', 'two')\n",
      "V= 80120\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "bi_grams=[]\n",
    "for sent in sentences:\n",
    "    gram2=ngrams(sent,2)\n",
    "    bi_grams.extend(gram2)\n",
    "print(len(bi_grams))\n",
    "\n",
    "for i in range(3):\n",
    "    print(bi_grams[i])\n",
    "\n",
    "counter_bigram = Counter(bi_grams)\n",
    "print('V=',len(counter_bigram))\n",
    "print(counter_bigram[('here','are')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185925\n",
      "('<s>', 'here', 'are')\n",
      "('here', 'are', 'two')\n",
      "('are', 'two', 'reasons')\n",
      "V= 142550\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tri_grams=[]\n",
    "for sent in sentences:\n",
    "    gram3=ngrams(sent,3)\n",
    "    tri_grams.extend(gram3)\n",
    "print(len(tri_grams))\n",
    "\n",
    "for i in range(3):\n",
    "    print(tri_grams[i])\n",
    "\n",
    "counter_trigram = Counter(tri_grams)\n",
    "print('V=',len(counter_trigram))\n",
    "print(counter_trigram[('here','are','two')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175929\n",
      "('<s>', 'here', 'are', 'two')\n",
      "('here', 'are', 'two', 'reasons')\n",
      "('are', 'two', 'reasons', 'companies')\n",
      "V = 163392\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "four_grams=[]\n",
    "for sent in sentences:\n",
    "    gram4=ngrams(sent,4)\n",
    "    four_grams.extend(gram4)\n",
    "print(len(four_grams))\n",
    "\n",
    "for i in range(3):\n",
    "    print(four_grams[i])\n",
    "counter_fourgram = Counter(four_grams)\n",
    "print('V =',len(counter_fourgram))\n",
    "print(counter_fourgram[('here','are','two','reasons')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tính theo laplace\n",
    "# alpha = 0.001\n",
    "# def uni_prob(word):\n",
    "#     return max(1,counter_unigram[word]+alpha)/all_token_count\n",
    "# def bi_prob(word1,word2):\n",
    "#     return (counter_bigram[(word1,word2)]+alpha)/(counter_unigram[word1]+V)\n",
    "# def tri_prob(word1,word2,word3):\n",
    "#     return (counter_trigram[(word1,word2,word3)]+alpha)/(counter_bigram[(word1,word2)]+1+V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# backoff\n",
    "# tính prob theo từng loại: 1-gram, 2-gram, 3-gram\n",
    "def uni_prob(word):\n",
    "    return max(1,counter_unigram[word])/all_tokens_count\n",
    "\n",
    "def bi_prob(word1,word2):\n",
    "    if counter_bigram[(word1,word2)]>0:\n",
    "        return counter_bigram[(word1,word2)]/counter_unigram[word1]\n",
    "    else:\n",
    "        return 0.4*uni_prob(word2)\n",
    "    \n",
    "def tri_prob(word1,word2,word3):\n",
    "    if counter_trigram[(word1,word2,word3)]>0:\n",
    "        return counter_trigram[(word1,word2,word3)]/counter_bigram[(word1,word2)]\n",
    "    else:\n",
    "        return 0.4*bi_prob(word1,word2)\n",
    "print(tri_prob('here','are','two'))\n",
    "def four_prob(word1,word2,word3,word4):\n",
    "    if counter_fourgram[(word1,word2,word3,word4)]>0:\n",
    "        return counter_fourgram[(word1,word2,word3,word4)]/counter_trigram[(word1,word2,word3)]\n",
    "    else:\n",
    "        return 0.4*tri_prob(word1,word2,word3)\n",
    "print(four_prob('here','are','two','reasons'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tính xác suất của một câu, normalize theo 1 từ \n",
    "def probLM(sent,n):\n",
    "    if n>4 or n<1: # không xét trường hợp này \n",
    "        return 0\n",
    "    tokens=nltk.word_tokenize(sent.lower())\n",
    "    tokens += ['<s>']+tokens\n",
    "    prob=1\n",
    "    for i in range(1,len(tokens)):\n",
    "        if n==1:\n",
    "            prob*=uni_prob(tokens[i])\n",
    "        elif n==2:\n",
    "            prob*=bi_prob(tokens[i-1],tokens[i])\n",
    "        elif n==3:\n",
    "            if i>=2:\n",
    "                prob*=tri_prob(tokens[i-2],tokens[i-1],tokens[i])\n",
    "            else:\n",
    "                prob*=bi_prob(tokens[i-1],tokens[i])\n",
    "    l=len(tokens)-1\n",
    "    return prob**(1/l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1\n",
      "prob= 0.0014115277343780864\n",
      "perplexity= 708.4522504551397\n",
      "n=2\n",
      "prob= 0.01850971039549826\n",
      "perplexity= 54.02569670907491\n",
      "n=3\n",
      "prob= 0.1577713762609277\n",
      "perplexity= 6.338285332227602\n"
     ]
    }
   ],
   "source": [
    "sent='the human body with new abilities is no longer a question'\n",
    "#sent='the human body with new from abilities is no longer a question'\n",
    "#sent='A few years back from'\n",
    "print('n=1')\n",
    "pr=probLM(sent,1)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)\n",
    "\n",
    "print('n=2')\n",
    "pr=probLM(sent,2)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)\n",
    "\n",
    "print('n=3')\n",
    "pr=probLM(sent,3)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob= 0.1577713762609277\n",
      "perplexity= 6.338285332227602\n",
      "prob= 0.00391831904286787\n",
      "perplexity= 255.21147947873246\n"
     ]
    }
   ],
   "source": [
    "# kiểm tra xem 2 xâu có xác suất hơn nhau thế nào, ví dụ cho bài toán speech to text\n",
    "sent1='the human body with new abilities is no longer a question'\n",
    "sent2='the human body with knew abilities is know longer a question'\n",
    "pr=probLM(sent1,3)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)\n",
    "\n",
    "pr=probLM(sent2,3)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasons\n"
     ]
    }
   ],
   "source": [
    "w1 = 'here'\n",
    "w2 = 'are'\n",
    "w3 = 'two'\n",
    "def get_word(w1,w2,w3):\n",
    "    p = 0\n",
    "    word =''\n",
    "    for w in counter_unigram.keys():\n",
    "        if p<four_prob(w1,w2,w3,w):\n",
    "            p = four_prob(w1,w2,w3,w)\n",
    "            word = w\n",
    "    return word\n",
    "print(get_word(w1,w2,w3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'function' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-71f230bf9e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'function' has no len()"
     ]
    }
   ],
   "source": [
    "def get_sent(w1,w2,w3,n):\n",
    "    sent=[w1,w2,w3]\n",
    "    while n>0:\n",
    "        for i in range(n):\n",
    "            sent.append(get_word(sent[i],sent[i+1],sent[i+2]))\n",
    "            n-=1\n",
    "    return sent\n",
    "get_sent(w1,w2,w3,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-ce7c98d3e87d>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-ce7c98d3e87d>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    frequences =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sentences = 'Exploitation is taking the knowledge we have and making good, better\\\n",
    " Exploitation is about making our trains run on time\\\n",
    " It\\'s about making good products faster and cheaper\\\n",
    " Exploitation is not risky -- in the short term\\\n",
    " But if we only exploit, it\\'s very risky in the long term\\\n",
    " And I think we all have memories of the famous pop groups who keep singing the same songs again and again, until they become obsolete or even pathetic\\\n",
    " So if we take a long-term perspective, we explore'\n",
    "string = nltk.word_tokenize(sentences)\n",
    "frequences = \n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
